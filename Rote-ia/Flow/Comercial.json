{"name": "Comercial", "description": "Language Models, Unleashed.", "icon": null, "is_component": false, "endpoint_name": null, "data": {"nodes": [{"id": "ChatInput-02hAw", "type": "genericNode", "position": {"x": 1662.803769337274, "y": 660.4951398645323}, "data": {"type": "ChatInput", "node": {"template": {"_type": "Component", "files": {"trace_as_metadata": true, "file_path": "", "fileTypes": ["txt", "md", "mdx", "csv", "json", "yaml", "yml", "xml", "html", "htm", "pdf", "docx", "py", "sh", "sql", "js", "ts", "tsx", "jpg", "jpeg", "png", "bmp", "image"], "list": true, "required": false, "placeholder": "", "show": true, "name": "files", "value": "", "display_name": "Files", "advanced": true, "dynamic": false, "info": "Files to be sent with the message.", "title_case": false, "type": "file", "_input_type": "FileInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "input_value": {"trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "Solicita\u00e7\u00e3o: Gostaria de conhecer os planos\ncliente_id: 1", "display_name": "Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Message to be passed as input.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}, "sender": {"trace_as_metadata": true, "options": ["Machine", "User"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "sender", "value": "User", "display_name": "Sender Type", "advanced": true, "dynamic": false, "info": "Type of sender.", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "sender_name": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "sender_name", "value": "User", "display_name": "Sender Name", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "Name of the sender.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "session_id": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "session_id", "value": "", "display_name": "Session ID", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The session ID of the chat. If empty, the current session ID parameter will be used.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "should_store_message": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "should_store_message", "value": true, "display_name": "Store Messages", "advanced": true, "dynamic": false, "info": "Store the message in the history.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}}, "description": "Get chat inputs from the Playground.", "icon": "ChatInput", "base_classes": ["Message"], "display_name": "Chat Input", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "message", "display_name": "Message", "method": "message_response", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value", "should_store_message", "sender", "sender_name", "session_id", "files"], "beta": false, "edited": false, "lf_version": "1.0.16"}, "id": "ChatInput-02hAw"}, "selected": false, "width": 384, "height": 297, "positionAbsolute": {"x": 1662.803769337274, "y": 660.4951398645323}, "dragging": false}, {"id": "ChatOutput-o7VEQ", "type": "genericNode", "position": {"x": 3105.250194163187, "y": 770.0242186081789}, "data": {"type": "ChatOutput", "node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_AI\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "data_template": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "data_template", "value": "{text}", "display_name": "Data Template", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "input_value": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Message to be passed as output.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "sender": {"trace_as_metadata": true, "options": ["Machine", "User"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "sender", "value": "Machine", "display_name": "Sender Type", "advanced": true, "dynamic": false, "info": "Type of sender.", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "sender_name": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "sender_name", "value": "AI", "display_name": "Sender Name", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "Name of the sender.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "session_id": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "session_id", "value": "", "display_name": "Session ID", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The session ID of the chat. If empty, the current session ID parameter will be used.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "should_store_message": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "should_store_message", "value": true, "display_name": "Store Messages", "advanced": true, "dynamic": false, "info": "Store the message in the history.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}}, "description": "Display a chat message in the Playground.", "icon": "ChatOutput", "base_classes": ["Message"], "display_name": "Chat Output", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "message", "display_name": "Message", "method": "message_response", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value", "should_store_message", "sender", "sender_name", "session_id", "data_template"], "beta": false, "edited": false, "lf_version": "1.0.16"}, "id": "ChatOutput-o7VEQ"}, "selected": false, "width": 384, "height": 297, "positionAbsolute": {"x": 3105.250194163187, "y": 770.0242186081789}, "dragging": false}, {"id": "ToolCallingAgent-yCBsz", "type": "genericNode", "position": {"x": 2630.0577078424817, "y": 392.69253980791643}, "data": {"type": "ToolCallingAgent", "node": {"template": {"_type": "Component", "chat_history": {"trace_as_metadata": true, "list": true, "trace_as_input": true, "required": false, "placeholder": "", "show": true, "name": "chat_history", "value": "", "display_name": "Chat History", "advanced": true, "input_types": ["Data"], "dynamic": false, "info": "", "title_case": false, "type": "other", "_input_type": "DataInput"}, "llm": {"trace_as_metadata": true, "list": false, "required": true, "placeholder": "", "show": true, "name": "llm", "value": "", "display_name": "Language Model", "advanced": false, "input_types": ["LanguageModel"], "dynamic": false, "info": "", "title_case": false, "type": "other", "_input_type": "HandleInput"}, "tools": {"trace_as_metadata": true, "list": true, "required": false, "placeholder": "", "show": true, "name": "tools", "value": "", "display_name": "Tools", "advanced": false, "input_types": ["Tool", "BaseTool"], "dynamic": false, "info": "", "title_case": false, "type": "other", "_input_type": "HandleInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from typing import Optional, List\n\nfrom langchain.agents import create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplate, HumanMessagePromptTemplate\nfrom langflow.base.agents.agent import LCToolsAgentComponent\nfrom langflow.inputs import MultilineInput\nfrom langflow.inputs.inputs import HandleInput, DataInput\nfrom langflow.schema import Data\n\n\nclass ToolCallingAgentComponent(LCToolsAgentComponent):\n    display_name: str = \"Tool Calling Agent\"\n    description: str = \"Agent that uses tools\"\n    icon = \"LangChain\"\n    beta = True\n    name = \"ToolCallingAgent\"\n\n    inputs = LCToolsAgentComponent._base_inputs + [\n        HandleInput(name=\"llm\", display_name=\"Language Model\", input_types=[\"LanguageModel\"], required=True),\n        MultilineInput(\n            name=\"system_prompt\",\n            display_name=\"System Prompt\",\n            info=\"System prompt for the agent.\",\n            value=\"You are a helpful assistant\",\n        ),\n        MultilineInput(\n            name=\"user_prompt\", display_name=\"Prompt\", info=\"This prompt must contain 'input' key.\", value=\"{input}\"\n        ),\n        DataInput(name=\"chat_history\", display_name=\"Chat History\", is_list=True, advanced=True),\n    ]\n\n    def get_chat_history_data(self) -> Optional[List[Data]]:\n        return self.chat_history\n\n    def create_agent_runnable(self):\n        if \"input\" not in self.user_prompt:\n            raise ValueError(\"Prompt must contain 'input' key.\")\n        messages = [\n            (\"system\", self.system_prompt),\n            (\"placeholder\", \"{chat_history}\"),\n            HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[\"input\"], template=self.user_prompt)),\n            (\"placeholder\", \"{agent_scratchpad}\"),\n        ]\n        prompt = ChatPromptTemplate.from_messages(messages)\n        return create_tool_calling_agent(self.llm, self.tools, prompt)\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "handle_parsing_errors": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "handle_parsing_errors", "value": true, "display_name": "Handle Parse Errors", "advanced": true, "dynamic": false, "info": "", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "input_value": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Input", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "max_iterations": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "max_iterations", "value": 15, "display_name": "Max Iterations", "advanced": true, "dynamic": false, "info": "", "title_case": false, "type": "int", "_input_type": "IntInput"}, "system_prompt": {"trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "system_prompt", "value": "", "display_name": "System Prompt", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "System prompt for the agent.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}, "user_prompt": {"trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "user_prompt", "value": "{input}", "display_name": "Prompt", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "This prompt must contain 'input' key.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}, "verbose": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "verbose", "value": true, "display_name": "Verbose", "advanced": true, "dynamic": false, "info": "", "title_case": false, "type": "bool", "_input_type": "BoolInput"}}, "description": "Agent that uses tools", "icon": "LangChain", "base_classes": ["AgentExecutor", "Message"], "display_name": "Tool Calling Agent", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["AgentExecutor"], "selected": "AgentExecutor", "name": "agent", "display_name": "Agent", "method": "build_agent", "value": "__UNDEFINED__", "cache": true}, {"types": ["Message"], "selected": "Message", "name": "response", "display_name": "Response", "method": "message_response", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value", "handle_parsing_errors", "verbose", "max_iterations", "tools", "llm", "system_prompt", "user_prompt", "chat_history"], "beta": true, "edited": false, "lf_version": "1.0.16"}, "id": "ToolCallingAgent-yCBsz"}, "selected": false, "width": 384, "height": 609, "dragging": false, "positionAbsolute": {"x": 2630.0577078424817, "y": 392.69253980791643}}, {"id": "OpenAIModel-U3SpM", "type": "genericNode", "position": {"x": 1650.310098657769, "y": 1040.735155522483}, "data": {"type": "OpenAIModel", "node": {"template": {"_type": "Component", "api_key": {"load_from_db": true, "required": false, "placeholder": "", "show": true, "name": "api_key", "value": null, "display_name": "OpenAI API Key", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The OpenAI API Key to use for the OpenAI model.", "title_case": false, "password": true, "type": "str", "_input_type": "SecretStrInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "import operator\nfrom functools import reduce\n\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "input_value": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Input", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MessageInput"}, "json_mode": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "json_mode", "value": false, "display_name": "JSON Mode", "advanced": true, "dynamic": false, "info": "If True, it will output JSON regardless of passing a schema.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "max_tokens": {"trace_as_metadata": true, "range_spec": {"step_type": "float", "min": 0, "max": 128000, "step": 0.1}, "list": false, "required": false, "placeholder": "", "show": true, "name": "max_tokens", "value": "", "display_name": "Max Tokens", "advanced": true, "dynamic": false, "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "model_kwargs": {"trace_as_input": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "model_kwargs", "value": {}, "display_name": "Model Kwargs", "advanced": true, "dynamic": false, "info": "", "title_case": false, "type": "dict", "_input_type": "DictInput"}, "model_name": {"trace_as_metadata": true, "options": ["gpt-4o-mini", "gpt-4o", "gpt-4-turbo", "gpt-4-turbo-preview", "gpt-4", "gpt-3.5-turbo", "gpt-3.5-turbo-0125"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "model_name", "value": "gpt-4o-mini", "display_name": "Model Name", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "openai_api_base": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "openai_api_base", "value": "", "display_name": "OpenAI API Base", "advanced": true, "dynamic": false, "info": "The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "output_schema": {"trace_as_input": true, "list": true, "required": false, "placeholder": "", "show": true, "name": "output_schema", "value": {}, "display_name": "Schema", "advanced": true, "dynamic": false, "info": "The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.", "title_case": false, "type": "dict", "_input_type": "DictInput"}, "seed": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "seed", "value": 1, "display_name": "Seed", "advanced": true, "dynamic": false, "info": "The seed controls the reproducibility of the job.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "stream": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "stream", "value": false, "display_name": "Stream", "advanced": true, "dynamic": false, "info": "Stream the response from the model. Streaming works only in Chat.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "system_message": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "system_message", "value": "", "display_name": "System Message", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "System message to pass to the model.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "temperature": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "temperature", "value": 0.1, "display_name": "Temperature", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "float", "_input_type": "FloatInput"}}, "description": "Generates text using OpenAI LLMs.", "icon": "OpenAI", "base_classes": ["LanguageModel", "Message"], "display_name": "OpenAI", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "text_output", "display_name": "Text", "method": "text_response", "value": "__UNDEFINED__", "cache": true}, {"types": ["LanguageModel"], "selected": "LanguageModel", "name": "model_output", "display_name": "Language Model", "method": "build_model", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value", "system_message", "stream", "max_tokens", "model_kwargs", "json_mode", "output_schema", "model_name", "openai_api_base", "api_key", "temperature", "seed"], "beta": false, "edited": true, "lf_version": "1.0.16"}, "id": "OpenAIModel-U3SpM"}, "selected": false, "width": 384, "height": 599, "positionAbsolute": {"x": 1650.310098657769, "y": 1040.735155522483}, "dragging": false}, {"id": "Plano_cliente-wPNfS", "type": "genericNode", "position": {"x": 1898.815957919996, "y": 275.81985144957696}, "data": {"type": "Plano_cliente", "node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.langchain_utilities.model import LCToolComponent\r\nfrom langflow.field_typing import Tool\r\nfrom langchain_core.tools import StructuredTool\r\nimport requests\r\n\r\nclass Plano_Cliente(LCToolComponent):\r\n    display_name = \"CRM da Skynet - Plano Cliente\"\r\n    description = \"Obtem dados do plano do cliente\"\r\n    name = \"Plano_cliente\"\r\n    icon = \"\ud83d\udc64\"\r\n\r\n    def build_tool(self) -> Tool:\r\n        def acesso_crm(cliente_id: int) -> str:\r\n            # Building the request URL\r\n            request_url = f\"https://jcnok-skynet.hf.space/clientes/{cliente_id}/plano/\"\r\n            request_planos = f\"https://jcnok-skynet.hf.space/planos/\"\r\n\r\n            try:\r\n                meu_plano = requests.get(request_url)\r\n                todos_planos = requests.get(request_planos)\r\n                meu_plano.raise_for_status()\r\n                todos_planos.raise_for_status()\r\n\r\n                # Extracting content from the response\r\n                content = meu_plano.json()  # Assuming the response is in JSON format\r\n                content1 = todos_planos.json()\r\n\r\n                # You can adjust this part depending on what the response contains\r\n                return f\"meu_plano={content}, planos={content1}\"\r\n\r\n            except requests.RequestException as e:\r\n                return f\"Erro ao verificar o cliente: {str(e)}\"\r\n        \r\n        return StructuredTool.from_function(\r\n            func=acesso_crm,\r\n            name=\"acesso_crm\",\r\n            description=\"Obtem dados a partir de um endpoint do CRM da Skynet sobre o plano do cliente\"\r\n        )", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}}, "description": "Obtem dados do plano do cliente", "icon": "\ud83d\udc64", "base_classes": ["Data", "Tool"], "display_name": "CRM da Skynet", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Data"], "selected": "Data", "name": "api_run_model", "display_name": "Data", "method": "run_model", "value": "__UNDEFINED__", "cache": true}, {"types": ["Tool"], "selected": "Tool", "name": "api_build_tool", "display_name": "Tool", "method": "build_tool", "value": "__UNDEFINED__", "cache": true}], "field_order": [], "beta": false, "edited": true, "lf_version": "1.0.16"}, "id": "Plano_cliente-wPNfS"}, "selected": false, "width": 384, "height": 256, "positionAbsolute": {"x": 1898.815957919996, "y": 275.81985144957696}, "dragging": false}, {"id": "Prompt-C03cb", "type": "genericNode", "position": {"x": 2159.2381376619674, "y": 886.6744351129582}, "data": {"type": "Prompt", "node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "template": {"trace_as_input": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "template", "value": "*Voc\u00ea tem apenas uma tool chamada de \"acesso_crm\"*\n\nVoc\u00ea \u00e9 o respons\u00e1vel pelo setor Comercial.\n\nO gerente ir\u00e1 te enviar uma solicita\u00e7\u00e3o do usu\u00e1rio e o `cliente_id`.\n\n1. Para identificar o plano do usu\u00e1rio, voc\u00ea consulta a tool \"acesso_crm\" (cliente_id: int): \n1.1 Acesse a tool \"acesso_crm\" com o `cliente_id`. \n\n1. Obtenha as informa\u00e7\u00f5es necess\u00e1rias como tipos de planos/velocidade oferecidos pela empresa, consultando a tool \"acesso_crm\" (cliente_id: str) com o cliente_id.\n\n2. Disponibilize o time comercial para fazer entrar em contato e ajustar a mudan\u00e7a do plano contratual, caso o usu\u00e1rio esteja interessado em algum dos upgrades oferecidos.\n\n3. Ofere\u00e7a um upgrade do plano atual do usu\u00e1rio. \n\n4. Inclua uma compara\u00e7\u00e3o de custos-benef\u00edcios entre o plano atual e os upgrades dispon\u00edveis.\n\nRespondo ao gerente instru\u00e7\u00f5es que devem ser passadas ao usu\u00e1rio, incluindo detalhes sobre planos maiores, poss\u00edveis combina\u00e7\u00f5es de servi\u00e7os adicionais, e os pre\u00e7os respectivos. Tamb\u00e9m, forne\u00e7a uma estimativa do tempo de resposta da equipe comercial ap\u00f3s a solicita\u00e7\u00e3o de upgrade e detalhe os servi\u00e7os adicionais oferecidos nos combos.\n\nSe voc\u00ea estiver interessado em um plano maior ou em combos, nossa equipe comercial pode entrar em contato para ajustar a mudan\u00e7a do plano\u00a0contratual. O prazo estimado de atendimento do setor comercial \u00e9 de at\u00e9 48h.", "display_name": "Template", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "prompt", "_input_type": "PromptInput"}}, "description": "Create a prompt template with dynamic variables.", "icon": "prompts", "is_input": null, "is_output": null, "is_composition": null, "base_classes": ["Message"], "name": "", "display_name": "Prompt", "documentation": "", "custom_fields": {"template": []}, "output_types": [], "full_path": null, "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "prompt", "hidden": null, "display_name": "Prompt Message", "method": "build_prompt", "value": "__UNDEFINED__", "cache": true}], "field_order": ["template"], "beta": false, "error": null, "edited": false}, "id": "Prompt-C03cb"}, "selected": false, "width": 384, "height": 325, "positionAbsolute": {"x": 2159.2381376619674, "y": 886.6744351129582}, "dragging": false}], "edges": [{"source": "ToolCallingAgent-yCBsz", "sourceHandle": "{\u0153dataType\u0153:\u0153ToolCallingAgent\u0153,\u0153id\u0153:\u0153ToolCallingAgent-yCBsz\u0153,\u0153name\u0153:\u0153response\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "ChatOutput-o7VEQ", "targetHandle": "{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153ChatOutput-o7VEQ\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_value", "id": "ChatOutput-o7VEQ", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "ToolCallingAgent", "id": "ToolCallingAgent-yCBsz", "name": "response", "output_types": ["Message"]}}, "id": "reactflow__edge-ToolCallingAgent-yCBsz{\u0153dataType\u0153:\u0153ToolCallingAgent\u0153,\u0153id\u0153:\u0153ToolCallingAgent-yCBsz\u0153,\u0153name\u0153:\u0153response\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-ChatOutput-o7VEQ{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153ChatOutput-o7VEQ\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "className": ""}, {"source": "ChatInput-02hAw", "sourceHandle": "{\u0153dataType\u0153:\u0153ChatInput\u0153,\u0153id\u0153:\u0153ChatInput-02hAw\u0153,\u0153name\u0153:\u0153message\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "ToolCallingAgent-yCBsz", "targetHandle": "{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153ToolCallingAgent-yCBsz\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_value", "id": "ToolCallingAgent-yCBsz", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "ChatInput", "id": "ChatInput-02hAw", "name": "message", "output_types": ["Message"]}}, "id": "reactflow__edge-ChatInput-02hAw{\u0153dataType\u0153:\u0153ChatInput\u0153,\u0153id\u0153:\u0153ChatInput-02hAw\u0153,\u0153name\u0153:\u0153message\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-ToolCallingAgent-yCBsz{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153ToolCallingAgent-yCBsz\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "className": ""}, {"source": "OpenAIModel-U3SpM", "sourceHandle": "{\u0153dataType\u0153:\u0153OpenAIModel\u0153,\u0153id\u0153:\u0153OpenAIModel-U3SpM\u0153,\u0153name\u0153:\u0153model_output\u0153,\u0153output_types\u0153:[\u0153LanguageModel\u0153]}", "target": "ToolCallingAgent-yCBsz", "targetHandle": "{\u0153fieldName\u0153:\u0153llm\u0153,\u0153id\u0153:\u0153ToolCallingAgent-yCBsz\u0153,\u0153inputTypes\u0153:[\u0153LanguageModel\u0153],\u0153type\u0153:\u0153other\u0153}", "data": {"targetHandle": {"fieldName": "llm", "id": "ToolCallingAgent-yCBsz", "inputTypes": ["LanguageModel"], "type": "other"}, "sourceHandle": {"dataType": "OpenAIModel", "id": "OpenAIModel-U3SpM", "name": "model_output", "output_types": ["LanguageModel"]}}, "id": "reactflow__edge-OpenAIModel-U3SpM{\u0153dataType\u0153:\u0153OpenAIModel\u0153,\u0153id\u0153:\u0153OpenAIModel-U3SpM\u0153,\u0153name\u0153:\u0153model_output\u0153,\u0153output_types\u0153:[\u0153LanguageModel\u0153]}-ToolCallingAgent-yCBsz{\u0153fieldName\u0153:\u0153llm\u0153,\u0153id\u0153:\u0153ToolCallingAgent-yCBsz\u0153,\u0153inputTypes\u0153:[\u0153LanguageModel\u0153],\u0153type\u0153:\u0153other\u0153}", "className": ""}, {"source": "Prompt-C03cb", "sourceHandle": "{\u0153dataType\u0153:\u0153Prompt\u0153,\u0153id\u0153:\u0153Prompt-C03cb\u0153,\u0153name\u0153:\u0153prompt\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "ToolCallingAgent-yCBsz", "targetHandle": "{\u0153fieldName\u0153:\u0153system_prompt\u0153,\u0153id\u0153:\u0153ToolCallingAgent-yCBsz\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "system_prompt", "id": "ToolCallingAgent-yCBsz", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "Prompt", "id": "Prompt-C03cb", "name": "prompt", "output_types": ["Message"]}}, "id": "reactflow__edge-Prompt-C03cb{\u0153dataType\u0153:\u0153Prompt\u0153,\u0153id\u0153:\u0153Prompt-C03cb\u0153,\u0153name\u0153:\u0153prompt\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-ToolCallingAgent-yCBsz{\u0153fieldName\u0153:\u0153system_prompt\u0153,\u0153id\u0153:\u0153ToolCallingAgent-yCBsz\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "className": ""}, {"source": "Plano_cliente-wPNfS", "sourceHandle": "{\u0153dataType\u0153:\u0153Plano_cliente\u0153,\u0153id\u0153:\u0153Plano_cliente-wPNfS\u0153,\u0153name\u0153:\u0153api_build_tool\u0153,\u0153output_types\u0153:[\u0153Tool\u0153]}", "target": "ToolCallingAgent-yCBsz", "targetHandle": "{\u0153fieldName\u0153:\u0153tools\u0153,\u0153id\u0153:\u0153ToolCallingAgent-yCBsz\u0153,\u0153inputTypes\u0153:[\u0153Tool\u0153,\u0153BaseTool\u0153],\u0153type\u0153:\u0153other\u0153}", "data": {"targetHandle": {"fieldName": "tools", "id": "ToolCallingAgent-yCBsz", "inputTypes": ["Tool", "BaseTool"], "type": "other"}, "sourceHandle": {"dataType": "Plano_cliente", "id": "Plano_cliente-wPNfS", "name": "api_build_tool", "output_types": ["Tool"]}}, "id": "reactflow__edge-Plano_cliente-wPNfS{\u0153dataType\u0153:\u0153Plano_cliente\u0153,\u0153id\u0153:\u0153Plano_cliente-wPNfS\u0153,\u0153name\u0153:\u0153api_build_tool\u0153,\u0153output_types\u0153:[\u0153Tool\u0153]}-ToolCallingAgent-yCBsz{\u0153fieldName\u0153:\u0153tools\u0153,\u0153id\u0153:\u0153ToolCallingAgent-yCBsz\u0153,\u0153inputTypes\u0153:[\u0153Tool\u0153,\u0153BaseTool\u0153],\u0153type\u0153:\u0153other\u0153}", "className": ""}], "viewport": {"x": -316.2915311214398, "y": -54.22955269380691, "zoom": 0.3373044143290697}}, "user_id": "9f3306fe-0011-43cf-8159-1d46ee55b16e", "folder_id": "453952ee-c984-4d19-98cd-a71160f9f9f5", "icon_bg_color": null, "updated_at": "2024-09-08T22:49:59+00:00", "webhook": false, "id": "d92d6bec-71b6-4e4f-a8d9-47b8555c709d"}