{"name": "Financeiro", "description": "Language Models, Unleashed.", "icon": null, "is_component": false, "endpoint_name": null, "data": {"nodes": [{"id": "ChatInput-oPX8k", "type": "genericNode", "position": {"x": 1222.3109586384473, "y": 658.5995285680827}, "data": {"type": "ChatInput", "node": {"template": {"_type": "Component", "files": {"trace_as_metadata": true, "file_path": "", "fileTypes": ["txt", "md", "mdx", "csv", "json", "yaml", "yml", "xml", "html", "htm", "pdf", "docx", "py", "sh", "sql", "js", "ts", "tsx", "jpg", "jpeg", "png", "bmp", "image"], "list": true, "required": false, "placeholder": "", "show": true, "name": "files", "value": "", "display_name": "Files", "advanced": true, "dynamic": false, "info": "Files to be sent with the message.", "title_case": false, "type": "file", "_input_type": "FileInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "input_value": {"trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "Solicita\u00e7\u00e3o: N\u00e3o recebi a fatura por email, preciso de uma segunda via\ncliente_id: 2", "display_name": "Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Message to be passed as input.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}, "sender": {"trace_as_metadata": true, "options": ["Machine", "User"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "sender", "value": "User", "display_name": "Sender Type", "advanced": true, "dynamic": false, "info": "Type of sender.", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "sender_name": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "sender_name", "value": "User", "display_name": "Sender Name", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "Name of the sender.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "session_id": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "session_id", "value": "", "display_name": "Session ID", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The session ID of the chat. If empty, the current session ID parameter will be used.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "should_store_message": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "should_store_message", "value": true, "display_name": "Store Messages", "advanced": true, "dynamic": false, "info": "Store the message in the history.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}}, "description": "Get chat inputs from the Playground.", "icon": "ChatInput", "base_classes": ["Message"], "display_name": "Chat Input", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "message", "display_name": "Message", "method": "message_response", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value", "should_store_message", "sender", "sender_name", "session_id", "files"], "beta": false, "edited": false, "lf_version": "1.0.16"}, "id": "ChatInput-oPX8k"}, "selected": false, "width": 384, "height": 297, "positionAbsolute": {"x": 1222.3109586384473, "y": 658.5995285680827}, "dragging": false}, {"id": "ChatOutput-IatDu", "type": "genericNode", "position": {"x": 2760.0103527216124, "y": 775.3998475401131}, "data": {"type": "ChatOutput", "node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_AI\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "data_template": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "data_template", "value": "{text}", "display_name": "Data Template", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "input_value": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Message to be passed as output.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "sender": {"trace_as_metadata": true, "options": ["Machine", "User"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "sender", "value": "Machine", "display_name": "Sender Type", "advanced": true, "dynamic": false, "info": "Type of sender.", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "sender_name": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "sender_name", "value": "AI", "display_name": "Sender Name", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "Name of the sender.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "session_id": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "session_id", "value": "", "display_name": "Session ID", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The session ID of the chat. If empty, the current session ID parameter will be used.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "should_store_message": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "should_store_message", "value": true, "display_name": "Store Messages", "advanced": true, "dynamic": false, "info": "Store the message in the history.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}}, "description": "Display a chat message in the Playground.", "icon": "ChatOutput", "base_classes": ["Message"], "display_name": "Chat Output", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "message", "display_name": "Message", "method": "message_response", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value", "should_store_message", "sender", "sender_name", "session_id", "data_template"], "beta": false, "edited": false, "lf_version": "1.0.16"}, "id": "ChatOutput-IatDu"}, "selected": false, "width": 384, "height": 297, "positionAbsolute": {"x": 2760.0103527216124, "y": 775.3998475401131}, "dragging": false}, {"id": "ToolCallingAgent-rEVyM", "type": "genericNode", "position": {"x": 2203.5849625323017, "y": 488.69506256172326}, "data": {"type": "ToolCallingAgent", "node": {"template": {"_type": "Component", "chat_history": {"trace_as_metadata": true, "list": true, "trace_as_input": true, "required": false, "placeholder": "", "show": true, "name": "chat_history", "value": "", "display_name": "Chat History", "advanced": true, "input_types": ["Data"], "dynamic": false, "info": "", "title_case": false, "type": "other", "_input_type": "DataInput"}, "llm": {"trace_as_metadata": true, "list": false, "required": true, "placeholder": "", "show": true, "name": "llm", "value": "", "display_name": "Language Model", "advanced": false, "input_types": ["LanguageModel"], "dynamic": false, "info": "", "title_case": false, "type": "other", "_input_type": "HandleInput"}, "tools": {"trace_as_metadata": true, "list": true, "required": false, "placeholder": "", "show": true, "name": "tools", "value": "", "display_name": "Tools", "advanced": false, "input_types": ["Tool", "BaseTool"], "dynamic": false, "info": "", "title_case": false, "type": "other", "_input_type": "HandleInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from typing import Optional, List\n\nfrom langchain.agents import create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplate, HumanMessagePromptTemplate\nfrom langflow.base.agents.agent import LCToolsAgentComponent\nfrom langflow.inputs import MultilineInput\nfrom langflow.inputs.inputs import HandleInput, DataInput\nfrom langflow.schema import Data\n\n\nclass ToolCallingAgentComponent(LCToolsAgentComponent):\n    display_name: str = \"Tool Calling Agent\"\n    description: str = \"Agent that uses tools\"\n    icon = \"LangChain\"\n    beta = True\n    name = \"ToolCallingAgent\"\n\n    inputs = LCToolsAgentComponent._base_inputs + [\n        HandleInput(name=\"llm\", display_name=\"Language Model\", input_types=[\"LanguageModel\"], required=True),\n        MultilineInput(\n            name=\"system_prompt\",\n            display_name=\"System Prompt\",\n            info=\"System prompt for the agent.\",\n            value=\"You are a helpful assistant\",\n        ),\n        MultilineInput(\n            name=\"user_prompt\", display_name=\"Prompt\", info=\"This prompt must contain 'input' key.\", value=\"{input}\"\n        ),\n        DataInput(name=\"chat_history\", display_name=\"Chat History\", is_list=True, advanced=True),\n    ]\n\n    def get_chat_history_data(self) -> Optional[List[Data]]:\n        return self.chat_history\n\n    def create_agent_runnable(self):\n        if \"input\" not in self.user_prompt:\n            raise ValueError(\"Prompt must contain 'input' key.\")\n        messages = [\n            (\"system\", self.system_prompt),\n            (\"placeholder\", \"{chat_history}\"),\n            HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[\"input\"], template=self.user_prompt)),\n            (\"placeholder\", \"{agent_scratchpad}\"),\n        ]\n        prompt = ChatPromptTemplate.from_messages(messages)\n        return create_tool_calling_agent(self.llm, self.tools, prompt)\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "handle_parsing_errors": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "handle_parsing_errors", "value": true, "display_name": "Handle Parse Errors", "advanced": true, "dynamic": false, "info": "", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "input_value": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Input", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "max_iterations": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "max_iterations", "value": 15, "display_name": "Max Iterations", "advanced": true, "dynamic": false, "info": "", "title_case": false, "type": "int", "_input_type": "IntInput"}, "system_prompt": {"trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "system_prompt", "value": "", "display_name": "System Prompt", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "System prompt for the agent.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}, "user_prompt": {"trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "user_prompt", "value": "{input}", "display_name": "Prompt", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "This prompt must contain 'input' key.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}, "verbose": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "verbose", "value": true, "display_name": "Verbose", "advanced": true, "dynamic": false, "info": "", "title_case": false, "type": "bool", "_input_type": "BoolInput"}}, "description": "Agent that uses tools", "icon": "LangChain", "base_classes": ["AgentExecutor", "Message"], "display_name": "Tool Calling Agent", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["AgentExecutor"], "selected": "AgentExecutor", "name": "agent", "display_name": "Agent", "method": "build_agent", "value": "__UNDEFINED__", "cache": true}, {"types": ["Message"], "selected": "Message", "name": "response", "display_name": "Response", "method": "message_response", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value", "handle_parsing_errors", "verbose", "max_iterations", "tools", "llm", "system_prompt", "user_prompt", "chat_history"], "beta": true, "edited": false, "lf_version": "1.0.16"}, "id": "ToolCallingAgent-rEVyM"}, "selected": false, "width": 384, "height": 609, "dragging": false}, {"id": "OpenAIModel-n1uTc", "type": "genericNode", "position": {"x": 1231.187831501983, "y": 1011.7624563929139}, "data": {"type": "OpenAIModel", "node": {"template": {"_type": "Component", "api_key": {"load_from_db": true, "required": false, "placeholder": "", "show": true, "name": "api_key", "value": null, "display_name": "OpenAI API Key", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The OpenAI API Key to use for the OpenAI model.", "title_case": false, "password": true, "type": "str", "_input_type": "SecretStrInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "import operator\nfrom functools import reduce\n\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "input_value": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Input", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MessageInput"}, "json_mode": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "json_mode", "value": false, "display_name": "JSON Mode", "advanced": true, "dynamic": false, "info": "If True, it will output JSON regardless of passing a schema.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "max_tokens": {"trace_as_metadata": true, "range_spec": {"step_type": "float", "min": 0, "max": 128000, "step": 0.1}, "list": false, "required": false, "placeholder": "", "show": true, "name": "max_tokens", "value": "", "display_name": "Max Tokens", "advanced": true, "dynamic": false, "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "model_kwargs": {"trace_as_input": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "model_kwargs", "value": {}, "display_name": "Model Kwargs", "advanced": true, "dynamic": false, "info": "", "title_case": false, "type": "dict", "_input_type": "DictInput"}, "model_name": {"trace_as_metadata": true, "options": ["gpt-4o-mini", "gpt-4o", "gpt-4-turbo", "gpt-4-turbo-preview", "gpt-4", "gpt-3.5-turbo", "gpt-3.5-turbo-0125"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "model_name", "value": "gpt-4o-mini", "display_name": "Model Name", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "openai_api_base": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "openai_api_base", "value": "", "display_name": "OpenAI API Base", "advanced": true, "dynamic": false, "info": "The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "output_schema": {"trace_as_input": true, "list": true, "required": false, "placeholder": "", "show": true, "name": "output_schema", "value": {}, "display_name": "Schema", "advanced": true, "dynamic": false, "info": "The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.", "title_case": false, "type": "dict", "_input_type": "DictInput"}, "seed": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "seed", "value": 1, "display_name": "Seed", "advanced": true, "dynamic": false, "info": "The seed controls the reproducibility of the job.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "stream": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "stream", "value": false, "display_name": "Stream", "advanced": true, "dynamic": false, "info": "Stream the response from the model. Streaming works only in Chat.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "system_message": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "system_message", "value": "", "display_name": "System Message", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "System message to pass to the model.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "temperature": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "temperature", "value": 0.1, "display_name": "Temperature", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "float", "_input_type": "FloatInput"}}, "description": "Generates text using OpenAI LLMs.", "icon": "OpenAI", "base_classes": ["LanguageModel", "Message"], "display_name": "OpenAI", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "text_output", "display_name": "Text", "method": "text_response", "value": "__UNDEFINED__", "cache": true}, {"types": ["LanguageModel"], "selected": "LanguageModel", "name": "model_output", "display_name": "Language Model", "method": "build_model", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value", "system_message", "stream", "max_tokens", "model_kwargs", "json_mode", "output_schema", "model_name", "openai_api_base", "api_key", "temperature", "seed"], "beta": false, "edited": true, "lf_version": "1.0.16"}, "id": "OpenAIModel-n1uTc"}, "selected": false, "width": 384, "height": 599, "positionAbsolute": {"x": 1231.187831501983, "y": 1011.7624563929139}, "dragging": false}, {"id": "Prompt-RzB5y", "type": "genericNode", "position": {"x": 1729.3332844627107, "y": 800.2963484222709}, "data": {"type": "Prompt", "node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "template": {"trace_as_input": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "template", "value": "Voce \u00e9 o respons\u00e1vel pelo setor Financeiro. \n\nO gerente ir\u00e1 te enviar uma solicita\u00e7\u00e3o do usu\u00e1rio e o `cliente_id` do usu\u00e1rio. \n\n1. Para identificar o plano do usu\u00e1rio, voc\u00ea consulta a tool \"acesso_crm\" (cliente_id: int): \n1.1 Acesse a tool \"acesso_crm\" com o `cliente_id`.\n\nAcesse a tool \"Gerar_Boleto\" e obtenha as informa\u00e7\u00f5es necess\u00e1rias como o n\u00famero do boleto para segunda via.\n\nResponda ao gerente todas instru\u00e7\u00f5es que devem ser passadas ao usu\u00e1rio, como um c\u00f3digo do boleto completo para que usu\u00e1rio j\u00e1 fa\u00e7a o pagamento no banco digital de sua prefer\u00eancia. Caso seja mencionado algo relacionado com email, n\u00e3o formatar a respota como sendo um email.\n\nA segunda via da fatura j\u00e1 \u00e9 o c\u00f3digo do boleto.", "display_name": "Template", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "prompt", "_input_type": "PromptInput"}}, "description": "Create a prompt template with dynamic variables.", "icon": "prompts", "is_input": null, "is_output": null, "is_composition": null, "base_classes": ["Message"], "name": "", "display_name": "Prompt", "documentation": "", "custom_fields": {"template": []}, "output_types": [], "full_path": null, "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "prompt", "hidden": null, "display_name": "Prompt Message", "method": "build_prompt", "value": "__UNDEFINED__", "cache": true}], "field_order": ["template"], "beta": false, "error": null, "edited": false}, "id": "Prompt-RzB5y"}, "selected": false, "width": 384, "height": 325, "positionAbsolute": {"x": 1729.3332844627107, "y": 800.2963484222709}, "dragging": false}, {"id": "Gerar_Boleto-1IDib", "type": "genericNode", "position": {"x": 1595.5861554682597, "y": 247.76921745224752}, "data": {"type": "Gerar_Boleto", "node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.field_typing import Tool\nfrom langchain_core.tools import StructuredTool\nimport requests\n\nclass CPFVerifier(LCToolComponent):\n    display_name = \"Gera_boteto_cliente\"\n    description = \"Verifica informa\u00e7\u00f5es do plano do cliente a partir do CPF do cliente.\"\n    name = \"Gerar_Boleto\"\n    icon = \"\ud83d\udc64\"\n\n\n    def build_tool(self) -> Tool:\n        def gerar_boleto(cliente_id: int) -> str:\n            # Definindo a URL para a solicita\u00e7\u00e3o POST\n            request_url = f\"https://jcnok-skynet.hf.space/financeiro/gerar-boleto/{cliente_id}\"\n            \n            # Definindo o payload que ser\u00e1 enviado no corpo da solicita\u00e7\u00e3o POST\n            # payload = {\"id_cliente\": id_cliente}  # Supondo que o servidor aceite um JSON com a chave 'id_cliente'\n\n            try:\n                # Fazendo a solicita\u00e7\u00e3o POST com o payload\n                response = requests.post(request_url)\n                response.raise_for_status()\n\n                # Extraindo o conte\u00fado da resposta\n                content = response.json()  # Supondo que a resposta seja em formato JSON\n\n                # Voc\u00ea pode ajustar esta parte dependendo do que a resposta cont\u00e9m\n                return f\"{content}\"\n\n            except requests.RequestException as e:\n                return f\"Erro ao verificar o cliente: {str(e)}\"\n        \n        return StructuredTool.from_function(\n            func=gerar_boleto,\n            name=\"acesso_crm\",\n            description=\"Gera o c\u00f3digo da fatura para oo cliente.\"\n    )", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}}, "description": "Verifica informa\u00e7\u00f5es do plano do cliente a partir do CPF do cliente.", "icon": "\ud83d\udc64", "base_classes": ["Data", "Tool"], "display_name": "CRM da Skynet", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Data"], "selected": "Data", "name": "api_run_model", "display_name": "Data", "method": "run_model", "value": "__UNDEFINED__", "cache": true}, {"types": ["Tool"], "selected": "Tool", "name": "api_build_tool", "display_name": "Tool", "method": "build_tool", "value": "__UNDEFINED__", "cache": true}], "field_order": [], "beta": false, "edited": true, "lf_version": "1.0.16"}, "id": "Gerar_Boleto-1IDib"}, "selected": false, "width": 384, "height": 284, "positionAbsolute": {"x": 1595.5861554682597, "y": 247.76921745224752}, "dragging": false}], "edges": [{"source": "ToolCallingAgent-rEVyM", "sourceHandle": "{\u0153dataType\u0153:\u0153ToolCallingAgent\u0153,\u0153id\u0153:\u0153ToolCallingAgent-rEVyM\u0153,\u0153name\u0153:\u0153response\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "ChatOutput-IatDu", "targetHandle": "{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153ChatOutput-IatDu\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_value", "id": "ChatOutput-IatDu", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "ToolCallingAgent", "id": "ToolCallingAgent-rEVyM", "name": "response", "output_types": ["Message"]}}, "id": "reactflow__edge-ToolCallingAgent-rEVyM{\u0153dataType\u0153:\u0153ToolCallingAgent\u0153,\u0153id\u0153:\u0153ToolCallingAgent-rEVyM\u0153,\u0153name\u0153:\u0153response\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-ChatOutput-IatDu{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153ChatOutput-IatDu\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "className": ""}, {"source": "ChatInput-oPX8k", "sourceHandle": "{\u0153dataType\u0153:\u0153ChatInput\u0153,\u0153id\u0153:\u0153ChatInput-oPX8k\u0153,\u0153name\u0153:\u0153message\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "ToolCallingAgent-rEVyM", "targetHandle": "{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153ToolCallingAgent-rEVyM\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_value", "id": "ToolCallingAgent-rEVyM", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "ChatInput", "id": "ChatInput-oPX8k", "name": "message", "output_types": ["Message"]}}, "id": "reactflow__edge-ChatInput-oPX8k{\u0153dataType\u0153:\u0153ChatInput\u0153,\u0153id\u0153:\u0153ChatInput-oPX8k\u0153,\u0153name\u0153:\u0153message\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-ToolCallingAgent-rEVyM{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153ToolCallingAgent-rEVyM\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "className": ""}, {"source": "OpenAIModel-n1uTc", "sourceHandle": "{\u0153dataType\u0153:\u0153OpenAIModel\u0153,\u0153id\u0153:\u0153OpenAIModel-n1uTc\u0153,\u0153name\u0153:\u0153model_output\u0153,\u0153output_types\u0153:[\u0153LanguageModel\u0153]}", "target": "ToolCallingAgent-rEVyM", "targetHandle": "{\u0153fieldName\u0153:\u0153llm\u0153,\u0153id\u0153:\u0153ToolCallingAgent-rEVyM\u0153,\u0153inputTypes\u0153:[\u0153LanguageModel\u0153],\u0153type\u0153:\u0153other\u0153}", "data": {"targetHandle": {"fieldName": "llm", "id": "ToolCallingAgent-rEVyM", "inputTypes": ["LanguageModel"], "type": "other"}, "sourceHandle": {"dataType": "OpenAIModel", "id": "OpenAIModel-n1uTc", "name": "model_output", "output_types": ["LanguageModel"]}}, "id": "reactflow__edge-OpenAIModel-n1uTc{\u0153dataType\u0153:\u0153OpenAIModel\u0153,\u0153id\u0153:\u0153OpenAIModel-n1uTc\u0153,\u0153name\u0153:\u0153model_output\u0153,\u0153output_types\u0153:[\u0153LanguageModel\u0153]}-ToolCallingAgent-rEVyM{\u0153fieldName\u0153:\u0153llm\u0153,\u0153id\u0153:\u0153ToolCallingAgent-rEVyM\u0153,\u0153inputTypes\u0153:[\u0153LanguageModel\u0153],\u0153type\u0153:\u0153other\u0153}", "className": ""}, {"source": "Prompt-RzB5y", "sourceHandle": "{\u0153dataType\u0153:\u0153Prompt\u0153,\u0153id\u0153:\u0153Prompt-RzB5y\u0153,\u0153name\u0153:\u0153prompt\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "ToolCallingAgent-rEVyM", "targetHandle": "{\u0153fieldName\u0153:\u0153system_prompt\u0153,\u0153id\u0153:\u0153ToolCallingAgent-rEVyM\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "system_prompt", "id": "ToolCallingAgent-rEVyM", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "Prompt", "id": "Prompt-RzB5y", "name": "prompt", "output_types": ["Message"]}}, "id": "reactflow__edge-Prompt-RzB5y{\u0153dataType\u0153:\u0153Prompt\u0153,\u0153id\u0153:\u0153Prompt-RzB5y\u0153,\u0153name\u0153:\u0153prompt\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-ToolCallingAgent-rEVyM{\u0153fieldName\u0153:\u0153system_prompt\u0153,\u0153id\u0153:\u0153ToolCallingAgent-rEVyM\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "className": ""}, {"source": "Gerar_Boleto-1IDib", "sourceHandle": "{\u0153dataType\u0153:\u0153Gerar_Boleto\u0153,\u0153id\u0153:\u0153Gerar_Boleto-1IDib\u0153,\u0153name\u0153:\u0153api_build_tool\u0153,\u0153output_types\u0153:[\u0153Tool\u0153]}", "target": "ToolCallingAgent-rEVyM", "targetHandle": "{\u0153fieldName\u0153:\u0153tools\u0153,\u0153id\u0153:\u0153ToolCallingAgent-rEVyM\u0153,\u0153inputTypes\u0153:[\u0153Tool\u0153,\u0153BaseTool\u0153],\u0153type\u0153:\u0153other\u0153}", "data": {"targetHandle": {"fieldName": "tools", "id": "ToolCallingAgent-rEVyM", "inputTypes": ["Tool", "BaseTool"], "type": "other"}, "sourceHandle": {"dataType": "Gerar_Boleto", "id": "Gerar_Boleto-1IDib", "name": "api_build_tool", "output_types": ["Tool"]}}, "id": "reactflow__edge-Gerar_Boleto-1IDib{\u0153dataType\u0153:\u0153Gerar_Boleto\u0153,\u0153id\u0153:\u0153Gerar_Boleto-1IDib\u0153,\u0153name\u0153:\u0153api_build_tool\u0153,\u0153output_types\u0153:[\u0153Tool\u0153]}-ToolCallingAgent-rEVyM{\u0153fieldName\u0153:\u0153tools\u0153,\u0153id\u0153:\u0153ToolCallingAgent-rEVyM\u0153,\u0153inputTypes\u0153:[\u0153Tool\u0153,\u0153BaseTool\u0153],\u0153type\u0153:\u0153other\u0153}", "className": ""}], "viewport": {"x": -176.4099928510292, "y": -71.79551586369638, "zoom": 0.3164281588569091}}, "user_id": "9f3306fe-0011-43cf-8159-1d46ee55b16e", "folder_id": "453952ee-c984-4d19-98cd-a71160f9f9f5", "icon_bg_color": null, "updated_at": "2024-09-08T22:50:17+00:00", "webhook": false, "id": "ca5d0de4-3eed-4d84-b7c8-967d758200b4"}